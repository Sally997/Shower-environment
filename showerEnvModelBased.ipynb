{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74159400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98b5d8e5-b700-41b3-83fa-d97a273021b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inheriting Env class from gym package, and overriding its methods\n",
    "class ShowerEnv(Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #actions are 0, 1, 2\n",
    "        self.action_space=Discrete(3)\n",
    "        #states are 5 discrete values 0, 1, 2,...\n",
    "        self.observation_space=Discrete(5)\n",
    "        #we have 60 seconds to adjust water's temprature\n",
    "        self.shower_length=60\n",
    "        #initial stat is random value from the states space\n",
    "        self.state=(self.observation_space.sample() + random.randint(-1,1))%5\n",
    "        #creating P dictionary to make it model based environment\n",
    "        self.P={}\n",
    "        #for each state we manage each possible action\n",
    "        for state in range(self.observation_space.n):\n",
    "            #this dict holds the results of applying an action to this state, holding all possible results\n",
    "            actions_states={}\n",
    "            #each possible action \n",
    "            for action in range(self.action_space.n):\n",
    "                #each action can result in one of the 3 possible states with an equal probability\n",
    "                new_state=[]\n",
    "                next_state=(state +(action - 1))%5\n",
    "                if next_state==2:\n",
    "                    reward=1\n",
    "                else:\n",
    "                    reward=0\n",
    "                new_state.append([0.33333333, next_state, reward, False])\n",
    "                #actions_states[action]=new_state\n",
    "                for s in range(0,2):\n",
    "                    #choose a random state\n",
    "                    next_state=(state+random.randint(-1,1))%5\n",
    "                    #print(next_state)\n",
    "                    #if the state is 2(warm water), then it's the goal state\n",
    "                    if next_state==2:\n",
    "                        reward=1\n",
    "                    else:\n",
    "                        reward=0\n",
    "                    new_state.append([0.33333333, next_state, reward, False])\n",
    "                actions_states[action]=new_state\n",
    "            self.P[state]=actions_states\n",
    "            \n",
    "            \n",
    "    #step function represents the interaction between the agent and the environment\n",
    "    def step(self, action):\n",
    "        #get the results of applying this action to the current state\n",
    "        states_list=self.P[self.state][action]\n",
    "        #choose random state from the states that can result in applying action to the current state\n",
    "        num_state=random.randint(0,2)\n",
    "        next_state=states_list[num_state][1]\n",
    "        reward=states_list[num_state][2]\n",
    "        #each step takes one second\n",
    "        self.shower_length -=1\n",
    "        #episode is over(game is over), when the time is over\n",
    "        if self.shower_length==0:\n",
    "            done=True\n",
    "        else:\n",
    "            done=False\n",
    "        \n",
    "        #set placeholder for info\n",
    "        info={}\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    #we call this function at the begining of each episode\n",
    "    def reset(self):\n",
    "        self.state= (self.observation_space.sample() + random.randint(0,1))%5\n",
    "        self.shower_length=60\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa5b0258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = ShowerEnv()\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e32b224e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2021b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.P[ 0 ][ 0 ] [[0.33333333, 4, 0, False], [0.33333333, 1, 0, False], [0.33333333, 4, 0, False]]\n",
      "env.P[ 0 ][ 1 ] [[0.33333333, 0, 0, False], [0.33333333, 1, 0, False], [0.33333333, 1, 0, False]]\n",
      "env.P[ 0 ][ 2 ] [[0.33333333, 1, 0, False], [0.33333333, 1, 0, False], [0.33333333, 4, 0, False]]\n",
      "env.P[ 1 ][ 0 ] [[0.33333333, 0, 0, False], [0.33333333, 2, 1, False], [0.33333333, 1, 0, False]]\n",
      "env.P[ 1 ][ 1 ] [[0.33333333, 1, 0, False], [0.33333333, 2, 1, False], [0.33333333, 2, 1, False]]\n",
      "env.P[ 1 ][ 2 ] [[0.33333333, 2, 1, False], [0.33333333, 1, 0, False], [0.33333333, 0, 0, False]]\n",
      "env.P[ 2 ][ 0 ] [[0.33333333, 1, 0, False], [0.33333333, 2, 1, False], [0.33333333, 3, 0, False]]\n",
      "env.P[ 2 ][ 1 ] [[0.33333333, 2, 1, False], [0.33333333, 3, 0, False], [0.33333333, 3, 0, False]]\n",
      "env.P[ 2 ][ 2 ] [[0.33333333, 3, 0, False], [0.33333333, 2, 1, False], [0.33333333, 3, 0, False]]\n",
      "env.P[ 3 ][ 0 ] [[0.33333333, 2, 1, False], [0.33333333, 4, 0, False], [0.33333333, 4, 0, False]]\n",
      "env.P[ 3 ][ 1 ] [[0.33333333, 3, 0, False], [0.33333333, 3, 0, False], [0.33333333, 2, 1, False]]\n",
      "env.P[ 3 ][ 2 ] [[0.33333333, 4, 0, False], [0.33333333, 3, 0, False], [0.33333333, 2, 1, False]]\n",
      "env.P[ 4 ][ 0 ] [[0.33333333, 3, 0, False], [0.33333333, 3, 0, False], [0.33333333, 3, 0, False]]\n",
      "env.P[ 4 ][ 1 ] [[0.33333333, 4, 0, False], [0.33333333, 4, 0, False], [0.33333333, 0, 0, False]]\n",
      "env.P[ 4 ][ 2 ] [[0.33333333, 0, 0, False], [0.33333333, 4, 0, False], [0.33333333, 3, 0, False]]\n"
     ]
    }
   ],
   "source": [
    "for state in range(env.observation_space.n):\n",
    "    for action in range(env.action_space.n): \n",
    "        print ('env.P[',state,'][',action,']',env.P[state][action]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b877693e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env.P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84dedaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration (env, gamma = 1.0) : \n",
    "    value_table = np. zeros(env. observation_space.n) \n",
    "    \n",
    "    no_of_iterations = 100000 \n",
    "    threshold = 1e-20 \n",
    "    for i in range (no_of_iterations) : \n",
    "        updated_value_table = np. copy(value_table) \n",
    "        #for each state in the space observation we're going to find the best possible value\n",
    "        for state in range(env. observation_space. n) : \n",
    "            Q_value = [] \n",
    "            #to find the best value we have to discover all possible actions and their corresponding values\n",
    "            for action in range(env. action_space. n) : \n",
    "                next_states_rewards = [] \n",
    "                #we'll use bellman optimality equation to find the optimal value table\n",
    "                #we do that by calculating the expected value of the current state considering all possible next states coming \n",
    "                #after taking the action\n",
    "                #for example, taking action a0 in state s0 can lead 0.2 to state s1, and 0.8 to state s2\n",
    "                #for that we loop over all possible rewards that can be gained using action a0 to find its value in state s0\n",
    "                #by that we're calculating state_action value (Q table)\n",
    "                for next_sr in env.P[state][action] : \n",
    "                    trans_prob, next_state, reward_prob,_ = next_sr \n",
    "                    next_states_rewards.append((trans_prob * (reward_prob + gamma * updated_value_table [next_state]))) \n",
    "                Q_value.append (np.sum (next_states_rewards) ) \n",
    "                # Pick up the maximum Q value and update it as value of a state \n",
    "                value_table[state] = max(Q_value) \n",
    "        #fabs() function is used to compute the absolute values element-wise\n",
    "        #if the difference between the old and updated value table is smaller or equal to the threshold,we reached convergence\n",
    "        if np.sum(np.fabs(updated_value_table - value_table)) <= threshold : \n",
    "            print('Value-iteration converged at iteration# %d. ' %(i+1)) \n",
    "            break \n",
    "    return value_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ee71ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting a policy from the optimal value table\n",
    "#we may have more than one optimal policy\n",
    "def extract_policy (value_table, gamma = 1.0) : \n",
    "    #initialize the policy as empty(zeroes)\n",
    "    policy = np.zeros(env. observation_space. n) \n",
    "    #for each state we're gonna compute the Q table that corresponds to it and all possible actions using the optimal value table\n",
    "    #that we calculated in a previous function\n",
    "    for state in range(env. observation_space. n):\n",
    "        Q_table = np. zeros(env. action_space. n) \n",
    "        #for each action calculate the q value using Bellman update equation\n",
    "        for action in range(env. action_space. n): \n",
    "            for next_sr in env.P[state][action]: \n",
    "                trans_prob, next_state, reward_prob,_ = next_sr \n",
    "                Q_table[action] += (trans_prob * (reward_prob + gamma * value_table[next_state])) \n",
    "        #now choose the action that has the maximum value using np. argmax which returns the indices of the maximum values\n",
    "        policy[state] = np. argmax(Q_table) \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fc08a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99944.26616251 99942.76616262 99950.01616225 99948.76616229\n",
      " 99947.26616233]\n"
     ]
    }
   ],
   "source": [
    "#we use gamma=1 because the observation space is small and the future rewards are quite important as the immediate ones\n",
    "x=value_iteration(env, gamma = 1.0) \n",
    "#always for episodic problems, the value of a terminal state is always zero,For the sake of consistent maths notation,\n",
    "#you can consider a terminal state to be \"absorbing\", i.e. any transition out of it results in zero reward and returning \n",
    "#to the same terminal state.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e85fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = extract_policy(x, gamma=1.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dbdbaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0ba07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfa29308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the outputs of this function demonstrates how many steps it took the agent before ending the game (the game can end when it:\n",
    "#reaches the goal, or falls in a hole)\n",
    "def get_score(env, optimal_policy, episodes=1000):\n",
    "  rewards=0\n",
    "  for episode in range(episodes):\n",
    "    #observation is the initial state, s0\n",
    "    observation = env.reset()\n",
    "    \n",
    "    while True:\n",
    "      action = optimal_policy[observation]\n",
    "      #When object interacts with environment with an action, then step() function returns observation which generally represents environment \n",
    "      #next state, reward a float of reward in previous action, done when it’s time to reset the environment or goal achieved \n",
    "      #and info a dict for debugging, it can be used for learning if it contains raw probabilities of environment’s last state.\n",
    "      observation, reward, done, _ = env.step(action)\n",
    "      \n",
    "      rewards+=reward\n",
    "      if done == 1:\n",
    "        #print('You are in the goal after {} steps'.format(steps))\n",
    "        #print('you managed to keep water temprature perfect for ', rewards,' second')\n",
    "        break\n",
    "  print('you kept water warm for ',rewards/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ab60a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you kept water warm for  322.46666666666664\n"
     ]
    }
   ],
   "source": [
    "episodes=1000\n",
    "get_score(env, optimal_policy, episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
